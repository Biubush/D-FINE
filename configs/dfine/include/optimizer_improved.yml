use_amp: True
use_ema: True
ema:
  type: ModelEMA
  decay: 0.9999
  warmups: 1000
  start: 0


epochs: 100
clip_max_norm: 0.1


optimizer:
  type: AdamW
  params:
    -
      params: '^(?=.*backbone)(?!.*norm).*$'
      lr: 0.0000075  # 降低骨干网络学习率
    -
      params: '^(?=.*backbone)(?=.*norm|bn).*$'
      lr: 0.0000075  # 统一降低
      weight_decay: 0.
    -
      params: '^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn)).*$'
      weight_decay: 0.

  lr: 0.00015  # 降低初始学习率，使训练更稳定
  betas: [0.9, 0.999]
  weight_decay: 0.0001  # 轻微增加权重衰减，减少过拟合


# 使用余弦退火学习率调度器替代多步长调度器
lr_scheduler:
  type: CosineAnnealingLR
  T_max: 100  # 与epochs保持一致
  eta_min: 0.00001  # 最小学习率

lr_warmup_scheduler:
  type: LinearWarmup
  warmup_duration: 600  # 增加预热周期 